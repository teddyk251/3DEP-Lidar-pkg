<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>3DEP-Lidar-pkg.src.metadata_generator API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>3DEP-Lidar-pkg.src.metadata_generator</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from logging import log
from urllib.request import urlopen
from json import dump, loads
from time import sleep
import pandas as pd
import sys
from copy import copy


def get_info(url_str: str) -&gt; tuple:
    &#34;&#34;&#34;Retrieve ept.json information from the AWS storage using python urlopen request

    Parameters
    ----------
    url_str : str
        URL to the ept.json file.

    Returns
    -------
    tuple
        A tuple with bounds, numbers of points, reprojection and schema information extracted from the ept.json file.
    &#34;&#34;&#34;
    try:
        # store the response of URL
        response = urlopen(url_str)
        # storing the JSON response
        # read data from response
        data_json = loads(response.read())

        return_data = data_json[&#39;bounds&#39;], data_json[&#39;points&#39;]

        print(
            &#39;Successfully Read and Retrieved Data Information from EPT.JSON File&#39;)

        # return extracted values
        return return_data

    except Exception as e:
        print(&#39;Failed To Read EPT.JSON File From URL&#39;)
        sys.exit(1)


def generate_dataset_metadata_json(directories_path: str = &#39;./data/region_list.txt&#39;, save: bool = False) -&gt; dict:
    &#34;&#34;&#34;Construct AWS Dataset Data Information. It extracts and identifies similar locations and organize them
    properly. It can also save the generated JSON file if needed.

    Parameters
    ----------
    directories_path : str, optional
        Path plus filename of the text file which contains the names of folders which are located in the AWS dataset storage.
        If filename of the directories is not given providing the file location is a must.
    save: bool, optional
        To save the generated json file in the same directory where the function was called.
    Returns
    -------
    dict
        The generated AWS Data Information in a json/dictionary format.
    &#34;&#34;&#34;

    main_url = &#34;https://usgs-lidar-public.s3.us-west-2.amazonaws.com/&#34;

    dataset_json = {}

    with open(directories_path, &#39;r&#39;) as locations:
        locations_list = locations.readlines()

    for index, location in enumerate(locations_list):
        try:
            location = location.replace(&#39;\n&#39;, &#34;&#34;)[:-1]
            folder_url = main_url + location + &#39;/ept.json&#39;
            bound, points, reprojection, schema = get_info(folder_url)
            location = location.split(&#39;_&#39;)
            if(&#39;LAS&#39; in location and location[location.index(&#39;LAS&#39;) - 1].isnumeric()):
                file_name = &#39;_&#39;.join(location[:-3])
                year = location[-3] + &#39;-&#39; + location[-1]
            else:
                file_name = &#39;_&#39;.join(location[:-1])
                year = location[-1]

            if(file_name not in dataset_json.keys()):
                new_file = {}
                new_file[&#39;bounds&#39;] = [bound]
                new_file[&#39;years&#39;] = [year]
                new_file[&#39;points&#39;] = [points]
                new_file[&#39;access_url&#39;] = [folder_url]
                new_file[&#39;len&#39;] = 1
                dataset_json[file_name] = new_file

            else:
                dict_value = dataset_json[file_name]
                dict_value[&#39;bounds&#39;].append(bound)
                dict_value[&#39;years&#39;].append(year)
                dict_value[&#39;points&#39;] = [points]
                dict_value[&#39;access_url&#39;].append(folder_url)
                dict_value[&#39;len&#39;] = dict_value[&#39;len&#39;] + 1

                dataset_json.update({file_name: dict_value})

            print(index, end=&#39;, &#39;)
            if(index % 100 == 0):
                sleep(5)

        except Exception as e:
            print(&#39;Failed To retrieve:\n\tfile_index -&gt; &#39;, index)
            print(&#34;Reason:\n\t -&gt; &#34;, e)
            continue

    if(save):
        with open(&#39;./dataset_metadata.json&#39;, &#39;w&#39;) as file_handler:
            dump(dataset_json, file_handler, sort_keys=True, indent=4)
    print(&#39;Successfully generated Data Information JSON File&#39;)

    return dataset_json


def get_values_list(json_data: dict) -&gt; tuple:
    &#34;&#34;&#34;Deconstructs the given dictionary values into specific data information values within the dictionary.

    Parameters
    ----------
    json_data : dict
        Dictionary data from which the data informations are extracted from.

    Returns
    -------
    tuple
        Tuple of lists for each extracted informations.
    &#34;&#34;&#34;
    try:
        file_names = list(json_data.keys())
        bounds_list = []
        points_list = []
        years_list = []
        access_list = []
        len_list = []
        for value in json_data.values():
            bounds_list.append(value[&#39;bounds&#39;])
            points_list.append(value[&#39;points&#39;])
            years_list.append(value[&#39;years&#39;])
            access_list.append(value[&#39;access_url&#39;])
            len_list.append(value[&#39;len&#39;])

        return_value = (file_names, bounds_list, points_list,
                        years_list, access_list, len_list)

        print(&#39;Successfully Retrieved Value Lists&#39;)

    except Exception as e:
        print(&#39;Failed to Retrieve Value Lists&#39;)

    return return_value


def merge_similar_bounds(json_data: dict, file_names: list, bounds_list: list) -&gt; dict:
    &#34;&#34;&#34;Finds keys in a dictionary where there bounds are similar and merges them.

    Parameters
    ----------
    json_data : dict
        Dictionary data from which the data informations were extracted from.
    file_names : list
        Keys list extracted from the Dictionary
    bounds_list : list
        Bounds list extracted from the Dictionary

    Returns
    -------
    dict
        New Dictionary where files with similar bounds are merged together.
    &#34;&#34;&#34;
    try:
        new_json = copy.deepcopy(json_data)

        check = []
        similar_values = []

        for index, i in enumerate(bounds_list):
            if i in check:
                similar_values.append([check.index(i), index])
                print(&#34;actual first index:&#34;, check.index(
                    i), &#34;bound index value:&#34;, index)
            else:
                check.append(i)

        for initial, later in similar_values:
            main_json = new_json[file_names[initial]]
            add_json = new_json[file_names[later]]

            new_file_name = f&#39;{file_names[initial]},{file_names[later]}&#39;
            new_file = {}
            new_file[&#39;bounds&#39;] = main_json[&#39;bounds&#39;]
            new_file[&#39;years&#39;] = main_json[&#39;years&#39;]
            new_file[&#39;years&#39;].extend(add_json[&#39;years&#39;])
            new_file[&#39;points&#39;] = main_json[&#39;points&#39;]
            new_file[&#39;points&#39;].extend(add_json[&#39;points&#39;])
            new_file[&#39;access_url&#39;] = main_json[&#39;access_url&#39;]
            new_file[&#39;access_url&#39;].extend(add_json[&#39;access_url&#39;])
            new_file[&#39;len&#39;] = main_json[&#39;len&#39;] + add_json[&#39;len&#39;]

            del new_json[file_names[initial]]
            del new_json[file_names[later]]

            new_json[new_file_name] = new_file

        print(&#39;Successfully merged files with related bounds&#39;)

    except Exception as e:
        print(&#39;Failed to merge bound related files&#39;)

    return new_json


def fix_bound_reptition_and_build_csv(json_data: dict, save: bool = True) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Fixes bound repition problems in a json files and builds a CSV representation of the JSON file.

    Parameters
    ----------
    json_data : dict
        Dictionary from which the CSV is built.
    save : bool, optional
        To save the generated CSV file with the name aws_dataset.csv in the current call path or not.

    Returns
    -------
    pd.DataFrame
        Pandas DataFrame representation of the JSON file with all bound similarities merged.
    &#34;&#34;&#34;
    file_names, bounds_list, points_list, reprojection_list, schema_list, years_list, access_list, len_list = get_values_list(
        json_data)

    final_json = merge_similar_bounds(json_data, file_names, bounds_list)

    file_names, bounds_list, points_list, years_list, access_list, len_list = get_values_list(
        final_json)

    aws_dataset_df = pd.DataFrame()
    aws_dataset_df[&#39;Region/s&#39;] = file_names
    aws_dataset_df[&#39;Bound/s&#39;] = bounds_list
    aws_dataset_df[&#39;NumberOfPoints&#39;] = points_list
    aws_dataset_df[&#39;Year/s&#39;] = years_list
    aws_dataset_df[&#39;Access Url/s&#39;] = access_list
    aws_dataset_df[&#39;Variations&#39;] = len_list

    if(save):
        aws_dataset_df.to_csv(&#39;./dataset_metadata.csv&#39;)

    print(
        &#39;Successfully Generated CSV file from JSON file applying bound merge fixes&#39;)

    return aws_dataset_df


if __name__ == &#34;__main__&#34;:
    final = generate_dataset_metadata_json()
    df = fix_bound_reptition_and_build_csv(final, save=False)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="3DEP-Lidar-pkg.src.metadata_generator.fix_bound_reptition_and_build_csv"><code class="name flex">
<span>def <span class="ident">fix_bound_reptition_and_build_csv</span></span>(<span>json_data: dict, save: bool = True) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Fixes bound repition problems in a json files and builds a CSV representation of the JSON file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>json_data</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary from which the CSV is built.</dd>
<dt><strong><code>save</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>To save the generated CSV file with the name aws_dataset.csv in the current call path or not.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>Pandas DataFrame representation of the JSON file with all bound similarities merged.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fix_bound_reptition_and_build_csv(json_data: dict, save: bool = True) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Fixes bound repition problems in a json files and builds a CSV representation of the JSON file.

    Parameters
    ----------
    json_data : dict
        Dictionary from which the CSV is built.
    save : bool, optional
        To save the generated CSV file with the name aws_dataset.csv in the current call path or not.

    Returns
    -------
    pd.DataFrame
        Pandas DataFrame representation of the JSON file with all bound similarities merged.
    &#34;&#34;&#34;
    file_names, bounds_list, points_list, reprojection_list, schema_list, years_list, access_list, len_list = get_values_list(
        json_data)

    final_json = merge_similar_bounds(json_data, file_names, bounds_list)

    file_names, bounds_list, points_list, years_list, access_list, len_list = get_values_list(
        final_json)

    aws_dataset_df = pd.DataFrame()
    aws_dataset_df[&#39;Region/s&#39;] = file_names
    aws_dataset_df[&#39;Bound/s&#39;] = bounds_list
    aws_dataset_df[&#39;NumberOfPoints&#39;] = points_list
    aws_dataset_df[&#39;Year/s&#39;] = years_list
    aws_dataset_df[&#39;Access Url/s&#39;] = access_list
    aws_dataset_df[&#39;Variations&#39;] = len_list

    if(save):
        aws_dataset_df.to_csv(&#39;./dataset_metadata.csv&#39;)

    print(
        &#39;Successfully Generated CSV file from JSON file applying bound merge fixes&#39;)

    return aws_dataset_df</code></pre>
</details>
</dd>
<dt id="3DEP-Lidar-pkg.src.metadata_generator.generate_dataset_metadata_json"><code class="name flex">
<span>def <span class="ident">generate_dataset_metadata_json</span></span>(<span>directories_path: str = './data/region_list.txt', save: bool = False) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Construct AWS Dataset Data Information. It extracts and identifies similar locations and organize them
properly. It can also save the generated JSON file if needed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>directories_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path plus filename of the text file which contains the names of folders which are located in the AWS dataset storage.
If filename of the directories is not given providing the file location is a must.</dd>
<dt><strong><code>save</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>To save the generated json file in the same directory where the function was called.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The generated AWS Data Information in a json/dictionary format.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_dataset_metadata_json(directories_path: str = &#39;./data/region_list.txt&#39;, save: bool = False) -&gt; dict:
    &#34;&#34;&#34;Construct AWS Dataset Data Information. It extracts and identifies similar locations and organize them
    properly. It can also save the generated JSON file if needed.

    Parameters
    ----------
    directories_path : str, optional
        Path plus filename of the text file which contains the names of folders which are located in the AWS dataset storage.
        If filename of the directories is not given providing the file location is a must.
    save: bool, optional
        To save the generated json file in the same directory where the function was called.
    Returns
    -------
    dict
        The generated AWS Data Information in a json/dictionary format.
    &#34;&#34;&#34;

    main_url = &#34;https://usgs-lidar-public.s3.us-west-2.amazonaws.com/&#34;

    dataset_json = {}

    with open(directories_path, &#39;r&#39;) as locations:
        locations_list = locations.readlines()

    for index, location in enumerate(locations_list):
        try:
            location = location.replace(&#39;\n&#39;, &#34;&#34;)[:-1]
            folder_url = main_url + location + &#39;/ept.json&#39;
            bound, points, reprojection, schema = get_info(folder_url)
            location = location.split(&#39;_&#39;)
            if(&#39;LAS&#39; in location and location[location.index(&#39;LAS&#39;) - 1].isnumeric()):
                file_name = &#39;_&#39;.join(location[:-3])
                year = location[-3] + &#39;-&#39; + location[-1]
            else:
                file_name = &#39;_&#39;.join(location[:-1])
                year = location[-1]

            if(file_name not in dataset_json.keys()):
                new_file = {}
                new_file[&#39;bounds&#39;] = [bound]
                new_file[&#39;years&#39;] = [year]
                new_file[&#39;points&#39;] = [points]
                new_file[&#39;access_url&#39;] = [folder_url]
                new_file[&#39;len&#39;] = 1
                dataset_json[file_name] = new_file

            else:
                dict_value = dataset_json[file_name]
                dict_value[&#39;bounds&#39;].append(bound)
                dict_value[&#39;years&#39;].append(year)
                dict_value[&#39;points&#39;] = [points]
                dict_value[&#39;access_url&#39;].append(folder_url)
                dict_value[&#39;len&#39;] = dict_value[&#39;len&#39;] + 1

                dataset_json.update({file_name: dict_value})

            print(index, end=&#39;, &#39;)
            if(index % 100 == 0):
                sleep(5)

        except Exception as e:
            print(&#39;Failed To retrieve:\n\tfile_index -&gt; &#39;, index)
            print(&#34;Reason:\n\t -&gt; &#34;, e)
            continue

    if(save):
        with open(&#39;./dataset_metadata.json&#39;, &#39;w&#39;) as file_handler:
            dump(dataset_json, file_handler, sort_keys=True, indent=4)
    print(&#39;Successfully generated Data Information JSON File&#39;)

    return dataset_json</code></pre>
</details>
</dd>
<dt id="3DEP-Lidar-pkg.src.metadata_generator.get_info"><code class="name flex">
<span>def <span class="ident">get_info</span></span>(<span>url_str: str) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve ept.json information from the AWS storage using python urlopen request</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>url_str</code></strong> :&ensp;<code>str</code></dt>
<dd>URL to the ept.json file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple with bounds, numbers of points, reprojection and schema information extracted from the ept.json file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_info(url_str: str) -&gt; tuple:
    &#34;&#34;&#34;Retrieve ept.json information from the AWS storage using python urlopen request

    Parameters
    ----------
    url_str : str
        URL to the ept.json file.

    Returns
    -------
    tuple
        A tuple with bounds, numbers of points, reprojection and schema information extracted from the ept.json file.
    &#34;&#34;&#34;
    try:
        # store the response of URL
        response = urlopen(url_str)
        # storing the JSON response
        # read data from response
        data_json = loads(response.read())

        return_data = data_json[&#39;bounds&#39;], data_json[&#39;points&#39;]

        print(
            &#39;Successfully Read and Retrieved Data Information from EPT.JSON File&#39;)

        # return extracted values
        return return_data

    except Exception as e:
        print(&#39;Failed To Read EPT.JSON File From URL&#39;)
        sys.exit(1)</code></pre>
</details>
</dd>
<dt id="3DEP-Lidar-pkg.src.metadata_generator.get_values_list"><code class="name flex">
<span>def <span class="ident">get_values_list</span></span>(<span>json_data: dict) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Deconstructs the given dictionary values into specific data information values within the dictionary.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>json_data</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary data from which the data informations are extracted from.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple of lists for each extracted informations.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_values_list(json_data: dict) -&gt; tuple:
    &#34;&#34;&#34;Deconstructs the given dictionary values into specific data information values within the dictionary.

    Parameters
    ----------
    json_data : dict
        Dictionary data from which the data informations are extracted from.

    Returns
    -------
    tuple
        Tuple of lists for each extracted informations.
    &#34;&#34;&#34;
    try:
        file_names = list(json_data.keys())
        bounds_list = []
        points_list = []
        years_list = []
        access_list = []
        len_list = []
        for value in json_data.values():
            bounds_list.append(value[&#39;bounds&#39;])
            points_list.append(value[&#39;points&#39;])
            years_list.append(value[&#39;years&#39;])
            access_list.append(value[&#39;access_url&#39;])
            len_list.append(value[&#39;len&#39;])

        return_value = (file_names, bounds_list, points_list,
                        years_list, access_list, len_list)

        print(&#39;Successfully Retrieved Value Lists&#39;)

    except Exception as e:
        print(&#39;Failed to Retrieve Value Lists&#39;)

    return return_value</code></pre>
</details>
</dd>
<dt id="3DEP-Lidar-pkg.src.metadata_generator.merge_similar_bounds"><code class="name flex">
<span>def <span class="ident">merge_similar_bounds</span></span>(<span>json_data: dict, file_names: list, bounds_list: list) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Finds keys in a dictionary where there bounds are similar and merges them.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>json_data</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary data from which the data informations were extracted from.</dd>
<dt><strong><code>file_names</code></strong> :&ensp;<code>list</code></dt>
<dd>Keys list extracted from the Dictionary</dd>
<dt><strong><code>bounds_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Bounds list extracted from the Dictionary</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>New Dictionary where files with similar bounds are merged together.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_similar_bounds(json_data: dict, file_names: list, bounds_list: list) -&gt; dict:
    &#34;&#34;&#34;Finds keys in a dictionary where there bounds are similar and merges them.

    Parameters
    ----------
    json_data : dict
        Dictionary data from which the data informations were extracted from.
    file_names : list
        Keys list extracted from the Dictionary
    bounds_list : list
        Bounds list extracted from the Dictionary

    Returns
    -------
    dict
        New Dictionary where files with similar bounds are merged together.
    &#34;&#34;&#34;
    try:
        new_json = copy.deepcopy(json_data)

        check = []
        similar_values = []

        for index, i in enumerate(bounds_list):
            if i in check:
                similar_values.append([check.index(i), index])
                print(&#34;actual first index:&#34;, check.index(
                    i), &#34;bound index value:&#34;, index)
            else:
                check.append(i)

        for initial, later in similar_values:
            main_json = new_json[file_names[initial]]
            add_json = new_json[file_names[later]]

            new_file_name = f&#39;{file_names[initial]},{file_names[later]}&#39;
            new_file = {}
            new_file[&#39;bounds&#39;] = main_json[&#39;bounds&#39;]
            new_file[&#39;years&#39;] = main_json[&#39;years&#39;]
            new_file[&#39;years&#39;].extend(add_json[&#39;years&#39;])
            new_file[&#39;points&#39;] = main_json[&#39;points&#39;]
            new_file[&#39;points&#39;].extend(add_json[&#39;points&#39;])
            new_file[&#39;access_url&#39;] = main_json[&#39;access_url&#39;]
            new_file[&#39;access_url&#39;].extend(add_json[&#39;access_url&#39;])
            new_file[&#39;len&#39;] = main_json[&#39;len&#39;] + add_json[&#39;len&#39;]

            del new_json[file_names[initial]]
            del new_json[file_names[later]]

            new_json[new_file_name] = new_file

        print(&#39;Successfully merged files with related bounds&#39;)

    except Exception as e:
        print(&#39;Failed to merge bound related files&#39;)

    return new_json</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="3DEP-Lidar-pkg.src" href="index.html">3DEP-Lidar-pkg.src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="3DEP-Lidar-pkg.src.metadata_generator.fix_bound_reptition_and_build_csv" href="#3DEP-Lidar-pkg.src.metadata_generator.fix_bound_reptition_and_build_csv">fix_bound_reptition_and_build_csv</a></code></li>
<li><code><a title="3DEP-Lidar-pkg.src.metadata_generator.generate_dataset_metadata_json" href="#3DEP-Lidar-pkg.src.metadata_generator.generate_dataset_metadata_json">generate_dataset_metadata_json</a></code></li>
<li><code><a title="3DEP-Lidar-pkg.src.metadata_generator.get_info" href="#3DEP-Lidar-pkg.src.metadata_generator.get_info">get_info</a></code></li>
<li><code><a title="3DEP-Lidar-pkg.src.metadata_generator.get_values_list" href="#3DEP-Lidar-pkg.src.metadata_generator.get_values_list">get_values_list</a></code></li>
<li><code><a title="3DEP-Lidar-pkg.src.metadata_generator.merge_similar_bounds" href="#3DEP-Lidar-pkg.src.metadata_generator.merge_similar_bounds">merge_similar_bounds</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>